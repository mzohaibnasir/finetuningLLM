{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb5BHa6l7JQC33Rztt/pMf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzohaibnasir/finetuningLLM/blob/main/02_QLoRA%26LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QLoRA and LoRA Quantization\n",
        "specifically used in fine tuning of LLMs.\n",
        "\n",
        "\n",
        "1. **Full Parameteric Fine tuning:** all weights are updated\n",
        "2. **Domain specific Fine tuning:** Finance/sales\n",
        "3. **Task specific tuning:** QnA chatbots, Document QnA chatbots\n"
      ],
      "metadata": {
        "id": "AkbKog4mzFnY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaKG5x3kyv4j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Parameteric Fine tuning:\n",
        "1. need to update all weights\n",
        "2. hardware resource constraints\n",
        "  1. Model Monitoring\n",
        "  2. Model inferencing\n",
        "  3. GPU\n",
        "  4. RAM\n",
        "\n",
        "\n",
        "\n",
        "**We use LoRA & QLoRA(LoRA 2.0) to oercome these challenges.**"
      ],
      "metadata": {
        "id": "lqsSWXFSqCwC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EBlBHfegzJPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA & QLoRA"
      ],
      "metadata": {
        "id": "09rXzwGYqn1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What does LoRA do?\n",
        " Inatead of updating weights it tracks changes.\n",
        "\n",
        "    Pre-trained weights + LoRA tracked weights = Fine-tuned weights\n",
        "\n",
        "\n",
        "  `LoRA tracked weights:(m*n)` are of same shape as `pretrained weights:(m*n)`. These two weights will be combine to get our fine-tuned weights."
      ],
      "metadata": {
        "id": "1HyYv7LxquGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume `n*m is 3*3` for both matrices. Due to matrix decomposition, our `3x3 LoRA tracked weights` would be stored in two small matrices of `1x3` and `3x1`. Resultant of these both small matrix will be actual `LoRA tracked weights`. So LoRA is decomposing `LoRA weight matrix` into  two smaller matrix based on ` Rank parameter`\n",
        "\n",
        "---\n",
        "\n",
        "#### Matrix decomposition:\n",
        "The rank of a matrix (denoted by œÅ(A)) is the maximum number of linearly independent rows (or columns) in the matrix."
      ],
      "metadata": {
        "id": "hNUo53kkrs1C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1SyOh-hLrhbs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}