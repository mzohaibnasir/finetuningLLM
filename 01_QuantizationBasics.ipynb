{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzvpWYpjCB5zEgUfMuKp6q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzohaibnasir/finetuningLLM/blob/main/01_QuantizationBasics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents"
      ],
      "metadata": {
        "id": "cOB2sFfhVpfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Fine tuning LLM\n",
        "2. Quantization\n",
        "  1. Full Precision/Half precision -> Data -> weights and parameters\n",
        "  2. Caliberation -> model optimization ->problems\n",
        "  3. Models of Quantiaztion\n",
        "    1. Post training Quantization\n",
        "    2. Quantization aware training"
      ],
      "metadata": {
        "id": "aY3Jb_KUSgUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization\n",
        "Conversion from higher memory format to lower memory format\n",
        "\n",
        "\n",
        "\n",
        " Quantization is indeed the process of converting a signal or data from a higher memory format (more bits) to a lower memory format (fewer bits). It essentially compresses the information by representing a wider range of values with a more limited set of discrete values.\n",
        "\n",
        "Here's a breakdown of what happens during quantization:\n",
        "\n",
        "1. **Input with high precision:**  This could be an analog signal (continuous values) or a digital signal with a large number of bits (e.g., 32-bit floating-point numbers).\n",
        "2. **Mapping to discrete levels:** The original values are mapped to a finite set of pre-defined levels. This often involves rounding or truncation, which introduces some loss of information.\n",
        "3. **Reduced memory footprint:** The resulting data uses fewer bits to represent each value, leading to a smaller overall size.\n",
        "\n",
        "This conversion is often a trade-off between:\n",
        "\n",
        "* **Accuracy:**  Higher precision formats provide more accurate representation, but require more memory.\n",
        "* **Efficiency:** Lower precision formats use less memory but may introduce errors due to quantization.\n",
        "\n",
        "Quantization is widely used in various applications:\n",
        "\n",
        "* **Digital Signal Processing (DSP):** Converting analog signals to digital form for processing often involves quantization.\n",
        "* **Image and Audio Compression:** JPEG images and MP3 audio use quantization techniques to reduce file size.\n",
        "* **Machine Learning:** Quantizing machine learning models can enable deployment on devices with limited memory resources.\n",
        "\n",
        "By understanding quantization, you can make informed decisions about balancing memory usage and accuracy in various applications.\n",
        "\n",
        "---\n",
        "\n",
        "## Full precision\n",
        "In ML jargon, FP32 is often termed “full precision or Single precision” (4 bytes).Each FP32 number would be stored based on 32-bits in memory.\n",
        "\n",
        "\n",
        "Lama2 has 70 billion parameters in terms of weights and bias.\n",
        "\n",
        "In Machine Learning (ML) jargon, FP32 is indeed referred to as both \"full precision\" and \"single precision.\" It utilizes 32 bits (4 bytes) to store each number, offering a high degree of accuracy for representing a wide range of values.\n",
        "\n",
        "\n",
        "fp32: full precision/single precision\n",
        "fp16: half precision\n",
        "\n",
        "\n",
        "---\n",
        "## So what if we convert model: FLoating point 32-bits -> int8\n",
        "\n",
        "Inference becoming quite easy this is called **QUANTIZATION**\n",
        "\n",
        "### Quantization makses inferenceing easy.\n",
        "\n",
        "You're on the right track! Converting a model from FP32 (32-bit floating-point) to INT8 (8-bit integer) through quantization does indeed make inference easier in several ways:\n",
        "\n",
        "* **Reduced Model Size:** Quantization significantly reduces the size of the model by representing weights and activations with fewer bits. This is because INT8 numbers use only 4 times less space compared to FP32 (1 byte vs 4 bytes).  A smaller model translates to:\n",
        "    * **Faster Loading:** Smaller models load faster onto devices, reducing the time it takes to get the model up and running for inference.\n",
        "    * **Lower Storage Requirements:**  Smaller models require less storage space on the device, which is crucial for resource-constrained devices like mobile phones.\n",
        "\n",
        "* **Faster Inference Speed:**  The hardware on many devices is optimized for integer computations rather than floating-point calculations. By converting to INT8, computations during inference become faster and more efficient, leading to quicker predictions.\n",
        "\n",
        "* **Potential for Wider Deployment:**  With a smaller size and faster inference speed, quantized models become suitable for deployment on a wider range of devices, including those with limited memory and processing power. This opens doors for applications on mobile devices, embedded systems, and even internet-of-things (IoT) devices.\n",
        "\n",
        "Here's a breakdown of your point:\n",
        "\n",
        "1. **Quantization makes inference easier:** Absolutely! By reducing model size and enabling faster computations, quantization simplifies the process of running the model for predictions on various devices.\n",
        "\n",
        "While quantization offers these advantages, it's important to remember:\n",
        "\n",
        "* **Potential Accuracy Loss:**  Quantization can introduce a slight decrease in accuracy due to the loss of precision during conversion from FP32 to INT8.\n",
        "* **Finding the Right Balance:** The key is to find the optimal precision level (e.g., INT8) that offers a good balance between efficiency and accuracy for your specific application.\n",
        "\n",
        "Overall, quantization is a powerful technique for making inference easier by enabling faster, more efficient model execution on various devices. It's a valuable tool for deploying machine learning models in real-world scenarios.\n",
        "\n"
      ],
      "metadata": {
        "id": "KpITmo8DPqJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**BUT THERE IS A TRADE off BETWEEN Efficiecncy and Accuracy**\n",
        "\n",
        "---\n",
        "**After quantization, you can further fine-tune the model to adapt to the reduced precision and improve its accuracy**"
      ],
      "metadata": {
        "id": "4dLqpnCEVJEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "wi97qFsQQL1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caliberation\n",
        "caliberation is how we'll able to conevert 32-bits model to 8-bit"
      ],
      "metadata": {
        "id": "_Qi_szMtVweI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rXStZJ7kVyo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to perform Quantization\n",
        "\n",
        "* Type of Quantizations\n",
        "  1. Symmertic Quantization\n",
        "      * Batch normalization is a technique of Symmetric quantization\n",
        "  2. Assymetric Quantization"
      ],
      "metadata": {
        "id": "14GR3-E_WDOE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HNLzQtJ3WKTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Symmetric Quantization (data is evenly distributed)\n",
        "\n"
      ],
      "metadata": {
        "id": "kthMhTRDW8Xv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Symmetric uint8 Quantization\n",
        "\n",
        "Assume your weight values ranges from 0  to 1000: [0.0,1000.0). These are weights for my LM. these are being stored in 32-bits. BTW weights have minimistic range\n",
        "\n",
        "**[0,0,  ... , 1000.0]**  ->  **weights**  ->  **32bits** -- **2^32** convert to **uint8**  --  **2^8(0-255)**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**So we'll be converting from [0,1000] -> [0,2550]**\n",
        "\n",
        "---\n",
        "\n",
        "### How Single Precision Floating point 32 is stored?\n",
        "\n",
        "1. 1-bit would be used to store sign\n",
        "2. next 8-bits for exponent\n",
        "3. next 23 for mantissa (anythingthat comes after decimal)\n",
        "\n",
        "\n",
        "for 7.32:\n",
        "1. for sign bit : +1\n",
        "2. for exponent: 7\n",
        "3. for mentissa: .32\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### How half Precision Floating point 16 is stored?\n",
        "1. 1-bit would be used to store sign\n",
        "2. next 5-bits for exponent\n",
        "3. next 10 for mantissa\n",
        "\n"
      ],
      "metadata": {
        "id": "6vKQJBR6XHqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FYSmCau4W-y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Min - Max scalar\n",
        "\n",
        "conversion is supposde to be like\n",
        "0.0 -> 0\n",
        "1000 -> 255\n",
        "\n",
        "\n",
        "so x range is 0 to 1000\n",
        "q range is 0 to 255\n",
        "\n",
        "\n",
        "    scalar = (xmax-xmin)/(qmax-qmin)\n",
        "\n",
        "      = (100 -0)/ (255-0) = 3.92\n",
        "\n",
        " here, `3.92` is a scale factor\n",
        "\n",
        "\n",
        " so if I divide `250` by scaling factor\n",
        "  round(250/3.92) = 64... this is new quantized value\n"
      ],
      "metadata": {
        "id": "XQexoykBHptT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Assymmetric uint8 Quantization (data skewiness in data distribution)\n",
        "\n",
        "values are [-20.0, ... , 1000.0]\n",
        "\n",
        "now to convert it into  uint 8:[0,255]\n",
        "\n",
        "    scalar = (xmax-xmin)/(qmax-qmin)\n",
        "\n",
        "    = (1000 -(-20)) / (255-0) = 4\n",
        "\n",
        "\n",
        "so, if we divide -20 by scaling factor.\n",
        "\n",
        "    (-20)/4 =  (-5) + (5) => 0  # 5 is zero point\n",
        "\n",
        "so, if we divide 1000 by scaling factor\n",
        "\n",
        "    (1000)/4 =  (250) + (5) => 255\n",
        "    \n",
        "\n",
        "\n",
        "Your explanation of asymmetric uint8 quantization is absolutely correct! You've accurately described the process of converting a range of values into the uint8 format (0 to 255) using scaling and zero-point adjustment.\n",
        "\n",
        "Here's a breakdown of the key points you mentioned:\n",
        "\n",
        "* **Asymmetric Quantization:** This type of quantization allows for different scales for positive and negative values. It's useful when the data distribution is skewed towards one direction, like in your example (-20.0 to 1000.0).\n",
        "* **Scaling Factor:** The scaling factor determines how much each value in the original range is stretched to fit the uint8 range. You've calculated it correctly as (xmax - xmin) / (qmax - qmin).\n",
        "* **Zero-Point:** The zero-point is an additive constant that helps map the minimum value (xmin) to the desired value in the uint8 range (usually 0). In your example, adding 5 to the scaled value ensures -20.0 maps to 0 in the uint8 range.\n",
        "\n",
        "Overall, your understanding of asymmetric uint8 quantization is on point!  Is there anything else you'd like to know about quantization or fine-tuning LLMs?"
      ],
      "metadata": {
        "id": "TWGrPClRbmS2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFjRE0PPZNmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So there are two important parameters required to perform quatization\n",
        "1. **zero point**\n",
        "  1. for symmetric quantization its 0 and for assymmetric it was +5\n",
        "2. **scale**\n",
        "  1. The scaling factor determines how much each value in the original range is stretched to fit the uint8 range\n",
        "\n",
        "\n",
        "\n",
        "  ---\n",
        "## Calibration\n",
        "  You're absolutely right! That's a perfect explanation of calibration in LLM quantization. Here's a breakdown of the key points:\n",
        "\n",
        "* **Quantization** focuses on compressing the model by reducing the precision of its weights and activations (e.g., from float32 to int8). This improves efficiency but can lead to a slight drop in accuracy.\n",
        "* **Calibration** comes after quantization and addresses the potential accuracy loss. It involves analyzing the activations of the model on a separate calibration dataset to understand the distribution of values.\n",
        "* Based on this analysis, calibration techniques determine optimal scaling factors and potentially zero-point adjustments for the quantized weights and activations. These adjustments ensure the quantized values closely represent the original high-precision values, minimizing accuracy degradation.\n",
        "\n",
        "Here's an analogy: Imagine you're shrinking high-resolution images (quantization). Calibration would be like adjusting brightness and contrast (scaling factors) on the compressed images to make them look closer to the originals.\n",
        "\n",
        "By effectively calibrating the model, you can achieve a good balance between model size/efficiency and accuracy when using quantized LLMs."
      ],
      "metadata": {
        "id": "W9vB2yHFgevC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_o6niXoFZNon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hmb8fckLZNqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LSiptrV2ZNtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwPQzGEyZNvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M8d6KvlBZNxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zX0PNqqIZN0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BmpGk8dnZN2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i6_PN1-pZN5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ye86XKQdZOKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Djmvfi-0ZOM4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}